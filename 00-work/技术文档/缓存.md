# 多级缓存
### 什么是多级缓存
多级缓存是一种**分层存储架构**，旨在通过组合不同存储介质的性能特性，构建成本与速度的最优平衡。
其核心架构通常包含三个层次：
- **L1 本地缓存（进程级）**：基于应用内存（如 Caffeine、Guava）。特点为**极低延迟**，适合拦截超高频热点请求。
- **L2 分布式缓存（集群级）**：基于高速中间件（如 Redis）。特点为**高并发、高可用**，用于在多节点间共享数据，作为本地缓存未命中时的快速回源。
- **L3 持久化存储（服务级）**：基于数据库或对象存储（如 MySQL、MinIO）。特点为**大容量、高可靠**，作为数据的最终落地与兜底保障。
 目标：**以空间换时间，以存储换计算**。用极低的缓存成本，屏蔽掉后端繁重的计算逻辑，实现系统的高吞吐与低延迟。
### 多级缓存与单级缓存的区别
| 维度 | 单级缓存 (Single-Level) | 多级缓存 (Multi-Level) | 核心差异解析 |
| :--- | :--- | :--- | :--- |
| **架构层次** | **扁平化**：仅依赖单一介质（通常为 Redis） | **立体化**：L1(本地) + L2(分布式) + L3(DB) | 多级缓存构建了存储梯度的速度与容量平衡。 |
| **性能 (RT)** | **中等**：受限于网络 I/O 与序列化开销 | **极致**：L1 命中时为内存纳秒级访问 | 多级缓存消除了高频热点数据的网络交互。 |
| **容量扩展** | **有限**：扩容需垂直升级或集群扩容，成本高 | **弹性**：利用 L1 内存存热点，L2 存温数据 | 实现了“小内存抗大流量”的高性价比扩展。 |
| **系统保护** | **单点风险**：缓存宕机直接穿透击垮后端 | **分层防御**：L2 宕机时 L1 仍能抵挡部分洪峰 | 增强了系统的容灾能力与抗雪崩能力。 |
| **命中率策略** | **静态均衡**：无法区分冷热数据 | **动态分层**：热点自动上浮至 L1，冷数据下沉 | 利用 LRU/LFU 算法实现了数据的自动分级存储。 |
### 为什么需要多级缓存
1. **防止“重复生成”导致的 CPU 算力浪费（防抖）**
    - **场景**：由于本地内存（L1）空间有限，旧 Key 被淘汰后，同一张热点瓦片被多个用户同时请求。
    - **痛点**：如果没有 L1 缓存拦截，大量请求会穿透到 Redis，甚至穿透到生成层。虽然有“生成后上传 MinIO”的逻辑，但在 MinIO 写入完成前的这段时间窗口内，**后端 TIFF 引擎会收到多次重复的切片计算指令**，导致 CPU 飙升。
    - **价值**：L1 本地缓存锁住了最热的 Key，确保对于已生成的瓦片，计算层**0 次重复计算**，彻底保护 TIFF 引擎。
2. **消除生成流程中的“MinIO 路径查询”延迟**
    - **场景**：用户请求一个坐标，系统需要先判断“这张图之前生成过没”。
    - **痛点**：判断“图是否存在”最准确的方法是查 MinIO，但这涉及一次网络 I/O。对于这种“可能存在也可能不存在”的检查请求，网络开销太重。
    - **价值**：通过 L1/L2 缓存 Key，**将“是否存在”的检查搬到了内存里**。如果命中 Key，直接去拉 MinIO 图；如果未命中 Key，直接走 TIFF 生成流程，省去了每次请求都要去 MinIO 探路的网络开销。
3. **利用“Key 遗忘”机制，让内存服务真正的“活跃数据”**
    - **场景**：TMS 瓦片数据量巨大（亿级 Key），但本地内存很小（只能存几万个 Key）。
    - **痛点**：如果只存 Redis，每次都要跨网络；如果只存本地，内存不够用，且无法区分哪些是“最近有人看的”，哪些是“几个月前看过一次的”。
    - **价值**：多级缓存利用 LRU/LFU 策略自动管理 Key。**只有最近被频繁访问的 Key 才会留在 L1**，偶尔被访问的 Key 降级到 Redis。这样，本地宝贵的内存始终只为当前最活跃的用户服务，实现了 Key 查询效率的最大化。
### 多级缓存的架构设计
结合 TMS 瓦片服务，常用架构：
#### 1. 第一层：本地内存缓存（L1）
- **存什么**：**最最热门的 Key 索引**。
- **角色**：它是系统的 **“极速反应部队”** 。因为是存在应用服务器的内存里，读取速度是纳秒级的，没有任何网络延迟。
- **作用**：专门拦截那些用户盯着不放的区域（比如市中心），确保这些重复请求连 Redis 都不用去问，直接就在本地搞定。
#### 2. 第二层：Redis 分布式缓存（L2）
- **存什么**：**近期访问过的 Key 索引**。
- **角色**：它是系统的 **“共享索引库”** 。
- **作用**：本地内存（L1）太小，存不下所有数据。L1 没找到时，就来 L2 找。Redis 是集群共享的，无论请求打到哪台服务器，只要这张图最近被生成过，这里都能查到路径，避免重复去“动”原始的 TIFF 文件。
#### 3. 第三层：MinIO 对象存储（L3）
- **存什么**：**所有的 PNG 瓦片文件**。
- **角色**：它是系统的 **“永久大仓库”** 。
- **作用**：这里容量无限大，存放着已经生成好的全量瓦片图片。如果 Redis 里也没这个 Key，说明这图很久没人看了，或者还没生成，这时候我们才考虑去动用最底层的 TIFF 引擎去“现做”一张。
> **访问流程**：请求 → 内存 → Redis → 数据库 → 原始数据生成
### 缓存策略
针对 Key 索引与瓦片生成的不同阶段，我们采用差异化的控制策略：
- **索引粒度**：**单瓦片 Key**
    - 采用最小粒度的 `Z/X/Y` 坐标作为缓存 Key（如 `tile:12:2048:1021`）。
    - **优势**：控制精细。只有在某个具体瓦片被请求时，才去检查索引或生成文件，避免加载不需要的区域数据，最大化节省内存和带宽。
- **Key 更新与淘汰策略**：
    - **热点管理（LRU/LFU）**：本地内存（L1）空间有限，采用 LRU（最近最少使用）或 LFU（最不经常使用）算法，**自动淘汰“冷门 Key”**，确保内存只保留用户最常访问的瓦片索引。
    - **主动刷新（重生成机制）**：当底层的 GeoTIFF 源文件数据更新时，通过消息队列或广播机制，**主动清除对应的 Key 缓存**。这会迫使下一次请求重新走生成流程，产出最新的瓦片并覆盖 MinIO 中的旧文件。
- **瓦片生成策略**：
    - **按需生成（默认策略）**：遵循“懒加载”原则，只有当 Key 在缓存中**不存在**时，才触发读取 TIFF 并切片转 PNG 的逻辑，避免冷数据浪费计算资源。
### 技术实现
- **L1 本地缓存：Hutool - LFU Cache**
    - **角色**：高频热点 Key 拦截器。
    - **实现原理**：利用 Hutool 提供的 `Cache` 接口（底层基于 `HashMap` 实现 LFU 策略）。
    - **优势**：
        - **极简集成**：Hutool 轻量级，API 友好，避免了 Caffeine 等重型库的复杂配置。
        - **热点识别**：**LFU（最不经常使用）算法**非常适合地图场景。它能自动识别并锁死那些**访问频率极高**的瓦片，即使这些热点在短时间内没有被访问，也不会轻易被淘汰。
    - **劣势**：基于 Java 堆内存，受限于 JVM 内存大小，不适合存储海量的冷数据 Key。
- **L2 分布式缓存：Redis**
    - **角色**：共享索引库。
    - **功能**：
        - **跨节点共享**：所有服务节点共享同一份 Key 索引，避免重复的 TIFF 切片计算。
        - **全量覆盖**：作为本地 LFU 缓存的后备，容量远大于本地内存，存储近期活跃的 Key。
        - **消息通知**：利用 Pub/Sub 机制，在底层数据更新时广播“清除 Key”指令。
- **L3 持久化存储：MinIO**
    - **角色**：瓦片文件仓库。
    - **功能**：
        - **海量存储**：低成本存储 PB 级别的 PNG 瓦片文件。
        - **数据持久化**：作为 TIFF 切片结果的最终落地点，支持长时间存储和历史查询。
### 挑战与应对
![[Pasted image 20251225144903.png]]
#### 总结

